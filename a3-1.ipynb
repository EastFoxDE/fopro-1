{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3b06b029-89c3-42fa-bcfc-5a12be2696fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18570.40472847 19359.19702213 19389.0047517  18023.30991097\n",
      " 19749.80542712 18493.26693958 18558.58218251 18234.23996759\n",
      " 19272.05500264 19325.69555466]\n",
      "0.003583192825317383\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "def show_testimage(test_image):\n",
    "    plt.imshow(test_image.squeeze(), cmap=\"gray\")\n",
    "    plt.title(f\"Label: {test_label}\")\n",
    "    plt.show()\n",
    "\n",
    "# Transformation: Bild in Tensor umwandeln\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "# Laden des Test-Datensatzes\n",
    "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
    "# Zugriff auf ein Testbild (z. B. das erste Bild)\n",
    "test_image, test_label = test_dataset[123]  # test_image ist ein Tensor, test_label ist die Ziffer\n",
    "\n",
    "# Testbild anzeigen\n",
    "#show_testimage(test_image)\n",
    "\n",
    "# Bild als Vektor p\n",
    "p_vec = test_image.view(784)\n",
    "p_vec = p_vec.detach().cpu().numpy() if isinstance(p_vec, torch.Tensor) else np.array(p_vec)\n",
    "\n",
    "# Implementieren Sie die Vorwärtsrechnung des Neuronalen Netzwerkes \n",
    "# o = F(p, v, w, a, b) und messen Sie die Zeit, die für die Berechnung benötigt wird!\n",
    "\n",
    "# Bias und Weights erzeugen\n",
    "v_vec = np.random.rand(784,10)\n",
    "w_vec = np.random.rand(784,784)\n",
    "a_vec = np.random.rand(784)\n",
    "b_vec = np.random.rand(10)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Layer berechnen\n",
    "h_vec = np.zeros(784)\n",
    "h_vec = np.dot(p_vec, w_vec) + a_vec\n",
    "h_vec = np.maximum(0, h_vec)  # ReLU\n",
    "o_vec = np.dot(h_vec, v_vec) + b_vec\n",
    "\n",
    "end = time.time()\n",
    "print(o_vec)\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cc9b3023-eec7-41fe-ab44-ba4478471002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ableitungen: [np.float64(0.0), np.float64(0.0), np.float64(49.52416218056674), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0), np.float64(0.0)]\n",
      "Zeit: 1.171 Sekunden\n"
     ]
    }
   ],
   "source": [
    "# Berechnen Sie die Ableitung des Netzwerkes nach einem Parameter vij im\n",
    "# Forward-Mode mit ihren differenzierbaren Datentypen aus Teil I!\n",
    "\n",
    "from diffType import diffType\n",
    "\n",
    "def forward_pass(p_vec, v_vec, w_vec, a_vec, b_vec, i, j):\n",
    "    \n",
    "    # Wandlung von p_vec in diffType (Ableitung 0)\n",
    "    p_vec = [diffType(val, 0.0) for val in p_vec]\n",
    "    \n",
    "    # Wandlung von w_vec und a_vec in diffType (Ableitung 0)\n",
    "    w_vec = [[diffType(val, 0.0) for val in row] for row in w_vec]\n",
    "    a_vec = [diffType(val, 0.0) for val in a_vec]\n",
    "    \n",
    "    # Wandlung von v_vec in diffType, wobei v_ij eine Ableitung von 1 hat\n",
    "    v_vec = [[diffType(val, 0.0) for val in row] for row in v_vec]\n",
    "    v_vec[i][j].dvalue = 1.0  # Setzt nur die gewünschte Ableitung auf 1\n",
    "    \n",
    "    # Wandlung von b_vec in diffType (Ableitung 0)\n",
    "    b_vec = [diffType(val, 0.0) for val in b_vec]\n",
    "    \n",
    "    # Layer-Berechnung\n",
    "    h_vec = [sum(p * w for p, w in zip(p_vec, w_row)) + a for w_row, a in zip(w_vec, a_vec)]\n",
    "    h_vec = [diffType(max(0, h.value), h.dvalue if h.value > 0 else 0.0) for h in h_vec]  # ReLU\n",
    "    o_vec = [sum(h * v for h, v in zip(h_vec, v_row)) + b for v_row, b in zip(zip(*v_vec), b_vec)]\n",
    "    \n",
    "    return o_vec  \n",
    "\n",
    "i, j = 5, 2  # Beispielhafter Index für v_ij\n",
    "start = time.time()\n",
    "result = forward_pass(p_vec, v_vec, w_vec, a_vec, b_vec, i, j)\n",
    "end = time.time()\n",
    "# Ausgabe der Ableitung von o nach v_ij\n",
    "print(\"Ableitungen:\", [o.dvalue for o in result])\n",
    "print(f\"Zeit: {end-start:.3f} Sekunden\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "049990d7-1e92-464a-abc2-3dcc2e2bf2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schätzen Sie, wie lange es dauern würde, den gesamten Gradientenvektor, d.h. alle partiellen Ableitungen von F\n",
    "# nach allen Parametern vij, wjk, aj, bk im Forward-Mode zu berechnen, und erstellen Sie eine Hochrechnung, \n",
    "# wie lange das Training mit dem MNIST-Datensatz über 1000 Epochen dauern würde!\n",
    "\n",
    "# 784²+7840+2×784 = 624064 Diff. notwendig\n",
    "# Ausführungszeit oben ca. 1.171 Sekunden\n",
    "# Gesamt also 730778,944 Sekunden \n",
    "# bzw. ca. 203h\n",
    "# Über 1000 Epochen = 1000 Schritte in Gradientenabstieg\n",
    "# demnach ca. 2030h +-\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4853864d-fb48-413a-87a8-cba90e62ca66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1355.33984375\n",
      "Gradienten von v: tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 45.6069,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 54.0785,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 43.4496,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 48.5394,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 50.4066,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 45.0292,  0.0000]])\n",
      "Loss: 1355.33984375\n",
      "Gradienten von v: tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 45.6069,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 54.0785,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 43.4496,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 48.5394,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 50.4066,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 45.0292,  0.0000]])\n",
      "Loss: 1355.33984375\n",
      "Gradienten von v: tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 45.6069,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 54.0785,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 43.4496,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 48.5394,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 50.4066,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 45.0292,  0.0000]])\n",
      "Loss: 1355.33984375\n",
      "Gradienten von v: tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 45.6069,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 54.0785,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 43.4496,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 48.5394,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 50.4066,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 45.0292,  0.0000]])\n",
      "Loss: 1355.33984375\n",
      "Gradienten von v: tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 45.6069,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 54.0785,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 43.4496,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 48.5394,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 50.4066,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000, 45.0292,  0.0000]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32176/3121635623.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_vec = torch.tensor(p_vec, dtype=torch.float32, requires_grad=False)\n",
      "/tmp/ipykernel_32176/3121635623.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  v_vec = torch.tensor(v_vec, dtype=torch.float32, requires_grad=True)  # Ableitung nach v\n",
      "/tmp/ipykernel_32176/3121635623.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  w_vec = torch.tensor(w_vec, dtype=torch.float32, requires_grad=True)  # Ableitung nach w\n",
      "/tmp/ipykernel_32176/3121635623.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a_vec = torch.tensor(a_vec, dtype=torch.float32, requires_grad=True)\n",
      "/tmp/ipykernel_32176/3121635623.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b_vec = torch.tensor(b_vec, dtype=torch.float32, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def forward_pass_torch(p_vec, v_vec, w_vec, a_vec, b_vec):\n",
    "    \"\"\"\n",
    "    Berechnet die Vorwärtspropagation mit PyTorch\n",
    "    \"\"\"\n",
    "    p_vec = torch.tensor(p_vec, dtype=torch.float32, requires_grad=False)\n",
    "    v_vec = torch.tensor(v_vec, dtype=torch.float32, requires_grad=True)  # Ableitung nach v\n",
    "    w_vec = torch.tensor(w_vec, dtype=torch.float32, requires_grad=True)  # Ableitung nach w\n",
    "    a_vec = torch.tensor(a_vec, dtype=torch.float32, requires_grad=True)\n",
    "    b_vec = torch.tensor(b_vec, dtype=torch.float32, requires_grad=True)\n",
    "    \n",
    "    h_vec = torch.relu(torch.matmul(p_vec, w_vec) + a_vec)  # Hidden Layer mit ReLU\n",
    "    o_vec = torch.matmul(h_vec, v_vec) + b_vec  # Output Layer\n",
    "    return o_vec, v_vec, w_vec, a_vec, b_vec\n",
    "\n",
    "def compute_loss_and_gradients(p_vec, v_vec, w_vec, a_vec, b_vec, target):\n",
    "    \"\"\"\n",
    "    Berechnet die Kreuzentropie-Verlustfunktion und deren Gradienten\n",
    "    \"\"\"\n",
    "    o_vec, v_vec, w_vec, a_vec, b_vec = forward_pass_torch(p_vec, v_vec, w_vec, a_vec, b_vec)\n",
    "    \n",
    "    # Softmax und Kreuzentropieverlust\n",
    "    loss = F.cross_entropy(o_vec.unsqueeze(0), torch.tensor([target], dtype=torch.long))\n",
    "    \n",
    "    # Gradienten berechnen\n",
    "    loss.backward()\n",
    "    \n",
    "    return loss.item(), v_vec.grad, w_vec.grad, a_vec.grad, b_vec.grad\n",
    "\n",
    "# Testdaten erzeugen\n",
    "p_vec = test_image.view(784).detach().cpu()\n",
    "v_vec = torch.rand(784, 10, requires_grad=True)\n",
    "w_vec = torch.rand(784, 784, requires_grad=True)\n",
    "a_vec = torch.rand(784, requires_grad=True)\n",
    "b_vec = torch.rand(10, requires_grad=True)\n",
    "\n",
    "target_label = test_label  # \"Truth\" welche Zahl gewählt wurde\n",
    "\n",
    "for i in range(0,5):\n",
    "    loss, dv, dw, da, db = compute_loss_and_gradients(p_vec, v_vec, w_vec, a_vec, b_vec, target_label)\n",
    "    print(\"Loss:\", loss)\n",
    "    print(\"Gradienten von v:\", dv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "71954645-d006-45da-8228-85b2392dd7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0\n",
      "Epoch 2, Loss: 0.0\n",
      "Epoch 3, Loss: 0.0\n",
      "Epoch 4, Loss: 0.0\n",
      "Epoch 5, Loss: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32176/2456677009.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  p_vec = torch.tensor(p_vec, dtype=torch.float32, requires_grad=False)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def forward_pass_torch(p_vec, v_vec, w_vec, a_vec, b_vec):\n",
    "    \"\"\"\n",
    "    Berechnet die Vorwärtspropagation mit PyTorch\n",
    "    \"\"\"\n",
    "    p_vec = torch.tensor(p_vec, dtype=torch.float32, requires_grad=False)\n",
    "    \n",
    "    h_vec = torch.relu(torch.matmul(p_vec, w_vec) + a_vec)  # Hidden Layer mit ReLU\n",
    "    o_vec = torch.matmul(h_vec, v_vec) + b_vec  # Output Layer\n",
    "    return o_vec\n",
    "\n",
    "def compute_loss(p_vec, v_vec, w_vec, a_vec, b_vec, target):\n",
    "    \"\"\"\n",
    "    Berechnet die Kreuzentropie-Verlustfunktion\n",
    "    \"\"\"\n",
    "    o_vec = forward_pass_torch(p_vec, v_vec, w_vec, a_vec, b_vec)\n",
    "    loss = F.cross_entropy(o_vec.unsqueeze(0), torch.tensor([target], dtype=torch.long))\n",
    "    return loss\n",
    "\n",
    "# Testdaten erzeugen\n",
    "p_vec = test_image.view(784).detach().cpu()\n",
    "v_vec = torch.rand(784, 10, requires_grad=True)\n",
    "w_vec = torch.rand(784, 784, requires_grad=True)\n",
    "a_vec = torch.rand(784, requires_grad=True)\n",
    "b_vec = torch.rand(10, requires_grad=True)\n",
    "\n",
    "target_label = 2  # Beispielhafte Zielklasse\n",
    "learning_rate = 0.01\n",
    "\n",
    "# Gradientenabstieg\n",
    "for epoch in range(5):  # 5 Iterationen\n",
    "    loss = compute_loss(p_vec, v_vec, w_vec, a_vec, b_vec, target_label)\n",
    "    \n",
    "    # Gradienten berechnen\n",
    "    loss.backward()\n",
    "    \n",
    "    # Parameter aktualisieren\n",
    "    with torch.no_grad():\n",
    "        v_vec -= learning_rate * v_vec.grad\n",
    "        w_vec -= learning_rate * w_vec.grad\n",
    "        a_vec -= learning_rate * a_vec.grad\n",
    "        b_vec -= learning_rate * b_vec.grad\n",
    "        \n",
    "        # Gradienten zurücksetzen\n",
    "        v_vec.grad.zero_()\n",
    "        w_vec.grad.zero_()\n",
    "        a_vec.grad.zero_()\n",
    "        b_vec.grad.zero_()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80edbe84-ca95-4f83-a2d5-d3ca27d1b883",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
